# ë°±ì—”ë“œ êµ¬í˜„ ê°€ì´ë“œ - OKKY Jobs í¬ë¡¤ë§ ì‹œìŠ¤í…œ

## ğŸ“‹ ê°œìš”
í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìš”êµ¬í•˜ëŠ” ì‹¤ì‹œê°„ í¬ë¡¤ë§ ë¡œê·¸ ë° íˆìŠ¤í† ë¦¬ ê¸°ëŠ¥ì„ ìœ„í•œ ë°±ì—”ë“œ API êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ”§ êµ¬í˜„ í•„ìš” API ì—”ë“œí¬ì¸íŠ¸

### 1. ê¸°ì¡´ API (ì´ë¯¸ êµ¬í˜„ë¨)
```
GET /okky/search          # ì±„ìš©ê³µê³  ê²€ìƒ‰
GET /okky/search/{id}     # ì±„ìš©ê³µê³  ìƒì„¸ ì¡°íšŒ
GET /okky/crawl/status    # í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ
POST /okky/crawl          # í¬ë¡¤ë§ ì‹¤í–‰
```

### 2. ìƒˆë¡œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” API

#### ğŸ“ í¬ë¡¤ë§ ë¡œê·¸ ê´€ë ¨

**ê¸°ë³¸ ë¡œê·¸ ì¡°íšŒ**
```http
GET /okky/crawl/logs
```

**Response:**
```json
{
  "success": true,
  "data": {
    "logs": [
      {
        "type": "info",
        "message": "í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "timestamp": "2025-10-23T11:20:00Z"
      },
      {
        "type": "success", 
        "message": "ë§ˆì§€ë§‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "timestamp": "2025-10-23T10:20:00Z"
      }
    ]
  }
}
```

**ì‹¤ì‹œê°„ ë¡œê·¸ ì¡°íšŒ**
```http
GET /okky/crawl/logs/realtime
```

**Response:**
```json
{
  "success": true,
  "data": {
    "logs": [
      {
        "type": "info",
        "message": "í¬ë¡¤ë§ ì‹œì‘: OKKY ì±„ìš©ê³µê³  ìˆ˜ì§‘",
        "timestamp": "2025-10-23T11:20:00Z"
      },
      {
        "type": "progress",
        "message": "í˜ì´ì§€ 1/10 ì²˜ë¦¬ ì¤‘... (15ê°œ ê³µê³  ìˆ˜ì§‘)",
        "timestamp": "2025-10-23T11:20:15Z",
        "progress": 10
      },
      {
        "type": "success",
        "message": "ì´ 150ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ",
        "timestamp": "2025-10-23T11:25:00Z"
      }
    ],
    "isRunning": true,
    "currentProgress": 10
  }
}
```

#### ğŸ“Š í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ê´€ë ¨

**í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì¡°íšŒ**
```http
GET /okky/crawl/history
```

**Response:**
```json
{
  "success": true,
  "data": {
    "history": [
      {
        "id": 1,
        "status": "ì™„ë£Œ",
        "startedAt": "2025-10-23T10:15:17Z",
        "endedAt": "2025-10-23T10:25:17Z",
        "duration": 600000,
        "processed": 150
      },
      {
        "id": 2,
        "status": "ì™„ë£Œ",
        "startedAt": "2025-10-23T09:15:17Z",
        "endedAt": "2025-10-23T09:25:17Z",
        "duration": 600000,
        "processed": 120
      }
    ]
  }
}
```

#### ğŸ›‘ í¬ë¡¤ë§ ì¤‘ì§€ ê´€ë ¨

**í¬ë¡¤ë§ ì¤‘ì§€**
```http
POST /okky/crawl/stop
```

**Response:**
```json
{
  "success": true,
  "message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
}
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸”
```sql
CREATE TABLE crawling_logs (
    id SERIAL PRIMARY KEY,
    type VARCHAR(20) NOT NULL,  -- info, success, error, warning, progress
    message TEXT NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    progress INTEGER DEFAULT NULL,  -- ì§„í–‰ë¥  (0-100)
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_crawling_logs_timestamp ON crawling_logs(timestamp);
CREATE INDEX idx_crawling_logs_type ON crawling_logs(type);
```

### í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ í…Œì´ë¸”
```sql
CREATE TABLE crawling_history (
    id SERIAL PRIMARY KEY,
    status VARCHAR(20) NOT NULL,  -- ì™„ë£Œ, ì‹¤íŒ¨, ì§„í–‰ì¤‘
    started_at TIMESTAMP WITH TIME ZONE NOT NULL,
    ended_at TIMESTAMP WITH TIME ZONE,
    duration INTEGER,  -- ë°€ë¦¬ì´ˆ
    processed INTEGER DEFAULT 0,  -- ì²˜ë¦¬ëœ í•­ëª© ìˆ˜
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_crawling_history_started_at ON crawling_history(started_at);
CREATE INDEX idx_crawling_history_status ON crawling_history(status);
```

## ğŸ”„ ì‹¤ì‹œê°„ ë¡œê·¸ êµ¬í˜„ ë°©ë²•

### ë°©ë²• 1: í´ë§ (Polling) - ê¶Œì¥

**Python FastAPI ì˜ˆì‹œ:**
```python
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
from datetime import datetime, timedelta

router = APIRouter()

@router.get("/crawl/logs")
async def get_crawling_logs(db: Session = Depends(get_db)):
    """ê¸°ë³¸ í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ"""
    logs = db.query(CrawlingLog).order_by(CrawlingLog.timestamp.desc()).limit(100).all()
    
    return {
        "success": True,
        "data": {
            "logs": [
                {
                    "type": log.type,
                    "message": log.message,
                    "timestamp": log.timestamp.isoformat()
                }
                for log in logs
            ]
        }
    }

@router.get("/crawl/logs/realtime")
async def get_realtime_logs(db: Session = Depends(get_db)):
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ"""
    # í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ í™•ì¸
    is_running = check_crawling_status()
    
    if is_running:
        # ìµœê·¼ 50ê°œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
        logs = db.query(CrawlingLog).order_by(CrawlingLog.timestamp.desc()).limit(50).all()
        progress = get_current_progress()
        
        return {
            "success": True,
            "data": {
                "logs": [
                    {
                        "type": log.type,
                        "message": log.message,
                        "timestamp": log.timestamp.isoformat(),
                        "progress": log.progress
                    }
                    for log in logs
                ],
                "isRunning": True,
                "currentProgress": progress
            }
        }
    else:
        return {
            "success": True,
            "data": {
                "logs": [],
                "isRunning": False,
                "currentProgress": 0
            }
        }

@router.get("/crawl/history")
async def get_crawling_history(db: Session = Depends(get_db)):
    """í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = db.query(CrawlingHistory).order_by(CrawlingHistory.started_at.desc()).limit(50).all()
    
    return {
        "success": True,
        "data": {
            "history": [
                {
                    "id": h.id,
                    "status": h.status,
                    "startedAt": h.started_at.isoformat(),
                    "endedAt": h.ended_at.isoformat() if h.ended_at else None,
                    "duration": h.duration,
                    "processed": h.processed
                }
                for h in history
            ]
        }
    }

@router.post("/crawl/stop")
async def stop_crawling():
    """í¬ë¡¤ë§ ì¤‘ì§€"""
    # í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ ë¡œì§
    stop_crawling_process()
    
    return {
        "success": True,
        "message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    }
```

### ë°©ë²• 2: WebSocket (ê³ ê¸‰)

**WebSocket ì‹¤ì‹œê°„ í†µì‹ :**
```python
from fastapi import WebSocket
import asyncio
import json

@app.websocket("/crawl/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    try:
        while True:
            # í¬ë¡¤ë§ ìƒíƒœ ëª¨ë‹ˆí„°ë§
            if is_crawling_running():
                logs = get_new_logs()
                await websocket.send_json({
                    "type": "log_update",
                    "data": logs
                })
            
            await asyncio.sleep(2)  # 2ì´ˆë§ˆë‹¤ ì²´í¬
    except WebSocketDisconnect:
        print("WebSocket ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
```

## ğŸ“ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¡œê·¸ ìƒì„±

### ë¡œê·¸ ìƒì„± í´ë˜ìŠ¤
```python
import logging
from datetime import datetime
from sqlalchemy.orm import Session

class CrawlingLogger:
    def __init__(self, db: Session):
        self.db = db
        self.logs = []
    
    def log_info(self, message: str):
        """ì •ë³´ ë¡œê·¸"""
        self.add_log("info", message)
    
    def log_progress(self, message: str, progress: int):
        """ì§„í–‰ë¥  ë¡œê·¸"""
        self.add_log("progress", message, progress)
    
    def log_success(self, message: str):
        """ì„±ê³µ ë¡œê·¸"""
        self.add_log("success", message)
    
    def log_error(self, message: str):
        """ì—ëŸ¬ ë¡œê·¸"""
        self.add_log("error", message)
    
    def log_warning(self, message: str):
        """ê²½ê³  ë¡œê·¸"""
        self.add_log("warning", message)
    
    def add_log(self, log_type: str, message: str, progress: int = None):
        """ë¡œê·¸ ì¶”ê°€"""
        log_entry = CrawlingLog(
            type=log_type,
            message=message,
            timestamp=datetime.now(),
            progress=progress
        )
        
        self.db.add(log_entry)
        self.db.commit()
        
        # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥ (ì‹¤ì‹œê°„ ì¡°íšŒìš©)
        self.logs.append({
            "type": log_type,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "progress": progress
        })
```

### í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‚¬ìš©
```python
def crawl_jobs(db: Session):
    """í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = CrawlingLogger(db)
    
    try:
        # í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì‹œì‘
        history = CrawlingHistory(
            status="ì§„í–‰ì¤‘",
            started_at=datetime.now()
        )
        db.add(history)
        db.commit()
        
        logger.log_info("í¬ë¡¤ë§ ì‹œì‘: OKKY ì±„ìš©ê³µê³  ìˆ˜ì§‘")
        
        total_processed = 0
        
        for page in range(1, 11):
            progress = page * 10
            logger.log_progress(f"í˜ì´ì§€ {page}/10 ì²˜ë¦¬ ì¤‘...", progress)
            
            # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§
            jobs = fetch_jobs_from_okky(page)
            total_processed += len(jobs)
            
            logger.log_info(f"{len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            save_jobs_to_db(jobs, db)
            
            # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
        
        # í¬ë¡¤ë§ ì™„ë£Œ
        history.status = "ì™„ë£Œ"
        history.ended_at = datetime.now()
        history.duration = int((history.ended_at - history.started_at).total_seconds() * 1000)
        history.processed = total_processed
        db.commit()
        
        logger.log_success(f"ì´ {total_processed}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
        
    except Exception as e:
        # ì—ëŸ¬ ì²˜ë¦¬
        logger.log_error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        if 'history' in locals():
            history.status = "ì‹¤íŒ¨"
            history.ended_at = datetime.now()
            history.duration = int((history.ended_at - history.started_at).total_seconds() * 1000)
            db.commit()
        
        raise e
```

## ğŸ“Š API ì‘ë‹µ í˜•ì‹ í†µì¼

### ì„±ê³µ ì‘ë‹µ
```json
{
  "success": true,
  "data": { ... },
  "message": "ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
}
```

### ì—ëŸ¬ ì‘ë‹µ
```json
{
  "success": false,
  "error": {
    "code": "CRAWLING_ERROR",
    "message": "í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    "details": "ìƒì„¸ ì—ëŸ¬ ì •ë³´"
  }
}
```

## ğŸ¯ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### 1ë‹¨ê³„ (í•„ìˆ˜) - ê¸°ë³¸ ê¸°ëŠ¥
- [ ] `GET /okky/crawl/logs` - ê¸°ë³¸ ë¡œê·¸ ì¡°íšŒ
- [ ] `GET /okky/crawl/history` - íˆìŠ¤í† ë¦¬ ì¡°íšŒ
- [ ] í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸” ìƒì„±
- [ ] í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ í…Œì´ë¸” ìƒì„±

### 2ë‹¨ê³„ (ê¶Œì¥) - ì‹¤ì‹œê°„ ê¸°ëŠ¥
- [ ] `GET /okky/crawl/logs/realtime` - ì‹¤ì‹œê°„ ë¡œê·¸
- [ ] `POST /okky/crawl/stop` - í¬ë¡¤ë§ ì¤‘ì§€
- [ ] í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ì— ë¡œê·¸ ìƒì„± ë¡œì§ ì¶”ê°€
- [ ] ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§

### 3ë‹¨ê³„ (ê³ ê¸‰) - ê³ ê¸‰ ê¸°ëŠ¥
- [ ] WebSocket ì‹¤ì‹œê°„ í†µì‹ 
- [ ] ë¡œê·¸ ë ˆë²¨ë³„ í•„í„°ë§
- [ ] ë¡œê·¸ ê²€ìƒ‰ ë° í˜ì´ì§•
- [ ] í¬ë¡¤ë§ ì„±ëŠ¥ ë©”íŠ¸ë¦­
- [ ] ë¡œê·¸ ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ

## ğŸ” í…ŒìŠ¤íŠ¸ ë°©ë²•

### API í…ŒìŠ¤íŠ¸
```bash
# ê¸°ë³¸ ë¡œê·¸ ì¡°íšŒ
curl -X GET "https://your-api-server.com/okky/crawl/logs"

# ì‹¤ì‹œê°„ ë¡œê·¸ ì¡°íšŒ
curl -X GET "https://your-api-server.com/okky/crawl/logs/realtime"

# í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
curl -X GET "https://your-api-server.com/okky/crawl/history"

# í¬ë¡¤ë§ ì¤‘ì§€
curl -X POST "https://your-api-server.com/okky/crawl/stop"
```

### í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ í™•ì¸
1. í¬ë¡¤ë§ ê´€ë¦¬ í˜ì´ì§€ì—ì„œ "í¬ë¡¤ë§ ì‹œì‘" ë²„íŠ¼ í´ë¦­
2. ì‹¤ì‹œê°„ ë¡œê·¸ê°€ 2ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸ë˜ëŠ”ì§€ í™•ì¸
3. í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ê°€ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸

## ğŸ“ ì°¸ê³ ì‚¬í•­

- í”„ë¡ íŠ¸ì—”ë“œëŠ” 2ì´ˆë§ˆë‹¤ ì‹¤ì‹œê°„ ë¡œê·¸ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤
- ë¡œê·¸ëŠ” ìµœê·¼ 50ê°œê¹Œì§€ë§Œ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤
- í¬ë¡¤ë§ì´ ì§„í–‰ ì¤‘ì¼ ë•Œë§Œ ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
- ì—ëŸ¬ ë°œìƒ ì‹œ ì ì ˆí•œ ì—ëŸ¬ ë¡œê·¸ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ ì‹œì—ë„ ë¡œê·¸ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ êµ¬í˜„í•˜ë©´ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í¬ë¡¤ë§ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
