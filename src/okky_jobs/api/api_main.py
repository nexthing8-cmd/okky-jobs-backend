from fastapi import FastAPI, Query, Request, HTTPException
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List, Dict, Any
from threading import Thread
import pymysql
import os
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from enum import Enum

from ..db.db import get_connection
from ..utils.excel_utils import export_to_excel
from ..utils.crawling_logger import CrawlingLogger
from ..scheduler.scheduler import job

# ì—´ê±°í˜• ì •ì˜
class Category(str, Enum):
    ê°œë°œ = "ê°œë°œ"
    ë””ìì¸ = "ë””ìì¸"
    ê¸°íš = "ê¸°íš"
    ë§ˆì¼€íŒ… = "ë§ˆì¼€íŒ…"
    ì˜ì—… = "ì˜ì—…"

class Location(str, Enum):
    ì„œìš¸ = "ì„œìš¸"
    ê²½ê¸° = "ê²½ê¸°"
    ì¸ì²œ = "ì¸ì²œ"
    ë¶€ì‚° = "ë¶€ì‚°"
    ëŒ€êµ¬ = "ëŒ€êµ¬"
    ê´‘ì£¼ = "ê´‘ì£¼"
    ëŒ€ì „ = "ëŒ€ì „"
    ìš¸ì‚° = "ìš¸ì‚°"
    ì„¸ì¢… = "ì„¸ì¢…"
    ê¸°íƒ€ = "ê¸°íƒ€"

class Experience(str, Enum):
    ì‹ ì… = "ì‹ ì…"
    ê²½ë ¥1_3ë…„ = "1-3ë…„"
    ê²½ë ¥3_5ë…„ = "3-5ë…„"
    ê²½ë ¥5_10ë…„ = "5-10ë…„"
    ê²½ë ¥10ë…„ì´ìƒ = "10ë…„ ì´ìƒ"

class Deadline(str, Enum):
    today = "today"
    three_days = "3days"
    one_week = "1week"
    one_month = "1month"

class SortBy(str, Enum):
    createdAt = "createdAt"
    company = "company"
    deadline = "deadline"
    views = "views"

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class JobSearchResult(BaseModel):
    id: str
    company: str
    title: str
    category: str
    location: str
    experience: str
    deadline: Optional[str]
    views: int
    createdAt: str
    updatedAt: str
    originalUrl: str

class JobDetail(BaseModel):
    id: str
    company: str
    title: str
    category: str
    location: str
    experience: str
    deadline: Optional[str]
    views: int
    createdAt: str
    updatedAt: str
    originalUrl: str
    description: str
    requirements: str
    techStack: List[str]
    contact: Dict[str, str]

class PaginationInfo(BaseModel):
    page: int
    limit: int
    total: int
    totalPages: int
    hasNext: bool
    hasPrev: bool

class SearchResponse(BaseModel):
    success: bool
    data: List[JobSearchResult]
    pagination: PaginationInfo
    filters: Dict[str, Any]

class JobDetailResponse(BaseModel):
    success: bool
    data: JobDetail

class StatsResponse(BaseModel):
    success: bool
    data: Dict[str, Any]

# ê²€ìƒ‰ ì¿¼ë¦¬ ë¹Œë”
def build_search_query(
    keyword: Optional[str] = None,
    category: Optional[str] = None,
    location: Optional[str] = None,
    experience: Optional[str] = None,
    deadline: Optional[str] = None,
    sort: str = "createdAt",
    page: int = 1,
    limit: int = 20
) -> tuple:
    """ê²€ìƒ‰ ì¡°ê±´ì— ë”°ë¥¸ SQL ì¿¼ë¦¬ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤."""
    
    # ê¸°ë³¸ ì¿¼ë¦¬ (ì‹¤ì œ í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
    base_query = """
    SELECT 
        j.id,
        j.company,
        j.title,
        j.category,
        j.location,
        j.career as experience,
        j.deadline,
        COALESCE(d.view_count, 0) as views,
        j.created_at,
        j.updated_at,
        j.link as original_url
    FROM okky_jobs j
    LEFT JOIN okky_job_details d ON j.link = d.link
    WHERE 1=1
    """
    
    count_query = "SELECT COUNT(*) as total FROM okky_jobs j LEFT JOIN okky_job_details d ON j.link = d.link WHERE 1=1"
    
    params = []
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ (ì œëª©, íšŒì‚¬ëª…ì—ì„œ ê²€ìƒ‰)
    if keyword:
        keyword_condition = """
        AND (j.title LIKE %s OR j.company LIKE %s)
        """
        base_query += keyword_condition
        count_query += keyword_condition
        keyword_param = f"%{keyword}%"
        params.extend([keyword_param, keyword_param])
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if category:
        base_query += " AND j.category = %s"
        count_query += " AND j.category = %s"
        params.append(category)
    
    # ì§€ì—­ í•„í„°
    if location:
        base_query += " AND j.location = %s"
        count_query += " AND j.location = %s"
        params.append(location)
    
    # ê²½ë ¥ í•„í„°
    if experience:
        base_query += " AND j.career = %s"
        count_query += " AND j.career = %s"
        params.append(experience)
    
    # ë§ˆê°ì¼ í•„í„° (deadlineì´ VARCHARì´ë¯€ë¡œ ë¬¸ìì—´ ë¹„êµ)
    if deadline:
        today = datetime.now().date()
        if deadline == "today":
            deadline_date = today.strftime("%Y-%m-%d")
        elif deadline == "3days":
            deadline_date = (today + timedelta(days=3)).strftime("%Y-%m-%d")
        elif deadline == "1week":
            deadline_date = (today + timedelta(days=7)).strftime("%Y-%m-%d")
        elif deadline == "1month":
            deadline_date = (today + timedelta(days=30)).strftime("%Y-%m-%d")
        else:
            deadline_date = today.strftime("%Y-%m-%d")
        
        # deadlineì´ VARCHARì´ë¯€ë¡œ ë¬¸ìì—´ ë¹„êµ ì‚¬ìš©
        base_query += " AND j.deadline <= %s"
        count_query += " AND j.deadline <= %s"
        params.append(deadline_date)
    
    # ì •ë ¬
    sort_mapping = {
        "createdAt": "j.created_at DESC",
        "company": "j.company ASC",
        "deadline": "j.deadline ASC",
        "views": "COALESCE(d.view_count, 0) DESC"
    }
    
    order_by = sort_mapping.get(sort, "j.created_at DESC")
    base_query += f" ORDER BY {order_by}"
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    offset = (page - 1) * limit
    base_query += f" LIMIT {limit} OFFSET {offset}"
    
    return base_query, count_query, params

# í™˜ê²½ì— ë”°ë¼ root_path ë™ì  ì„¤ì •
# ì„œë²„ ë°°í¬ ì‹œ: /okky (ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œìš©), ë¡œì»¬ ê°œë°œ ì‹œ: /
root_path = os.getenv("ROOT_PATH", "/okky")

app = FastAPI(
    title="OKKY ì±„ìš©ê³µê³  ê²€ìƒ‰ API", 
    version="1.0.0",
    root_path=root_path  # í™˜ê²½ ë³€ìˆ˜ë¡œ ë™ì  ì„¤ì •
)

# ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œë¥¼ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    # X-Forwarded-Prefix í—¤ë”ê°€ ìˆìœ¼ë©´ root_pathë¡œ ì‚¬ìš©
    forwarded_prefix = request.headers.get("X-Forwarded-Prefix")
    if forwarded_prefix:
        request.scope["root_path"] = forwarded_prefix
    response = await call_next(request)
    return response

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def search_jobs(keyword: Optional[str] = None) -> List[dict]:
    """DBì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜"""
    conn = get_connection()
    cursor = conn.cursor(pymysql.cursors.DictCursor)
    try:
        sql = """
            SELECT 
                j.title, j.company, j.link, j.deadline, j.category, j.position, j.location,
                j.career, j.salary,
                d.registered_at, d.view_count, d.start_date, d.work_location,
                d.pay_date, d.skill, d.description,
                c.name AS contact_name, c.phone AS contact_phone, c.email AS contact_email
            FROM okky_jobs j
            LEFT JOIN okky_job_details d ON j.link = d.link  -- âœ… ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸
            LEFT JOIN okky_job_contacts c ON d.contact_id = c.id
        """
        params = ()
        if keyword:
            sql += " WHERE j.title LIKE %s OR j.company LIKE %s OR d.description LIKE %s"
            params = (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%")
        sql += " ORDER BY j.created_at DESC"

        cursor.execute(sql, params)
        return cursor.fetchall()

    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []
    finally:
        cursor.close()
        conn.close()


#@app.on_event("startup")
#def startup_event():
#    Thread(target=job, daemon=True).start()


@app.get("/")
async def root():
    return {"message": "OKKY ì±„ìš©ê³µê³  ê²€ìƒ‰ APIì…ë‹ˆë‹¤. /jobs ë˜ëŠ” /jobs/export ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."}


@app.get("/jobs")
async def get_jobs(keyword: Optional[str] = Query(None)):
    rows = search_jobs(keyword)
    if not rows:
        return JSONResponse(content={"message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}, status_code=404)
    return JSONResponse(content=rows)

# ìƒˆë¡œìš´ ì±„ìš©ê³µê³  ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/search", response_model=SearchResponse)
async def search_jobs_new(
    keyword: Optional[str] = Query(None, description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    limit: int = Query(20, ge=1, le=100, description="í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜"),
    category: Optional[str] = Query(None, description="ì¹´í…Œê³ ë¦¬ í•„í„°"),
    location: Optional[str] = Query(None, description="ì§€ì—­ í•„í„°"),
    experience: Optional[str] = Query(None, description="ê²½ë ¥ í•„í„°"),
    deadline: Optional[str] = Query(None, description="ë§ˆê°ì¼ í•„í„° (today, 3days, 1week, 1month)"),
    sort: str = Query("createdAt", description="ì •ë ¬ ê¸°ì¤€ (createdAt, company, deadline, views)")
):
    """ì±„ìš©ê³µê³  ê²€ìƒ‰ API"""
    try:
        conn = get_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë¹Œë“œ
        search_query, count_query, params = build_search_query(
            keyword=keyword,
            category=category,
            location=location,
            experience=experience,
            deadline=deadline,
            sort=sort,
            page=page,
            limit=limit
        )
        
        # ì´ ê°œìˆ˜ ì¡°íšŒ
        cursor.execute(count_query, params)
        total = cursor.fetchone()['total']
        
        # ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ
        cursor.execute(search_query, params)
        results = cursor.fetchall()
        
        # ê²°ê³¼ ë³€í™˜
        jobs = []
        for row in results:
            jobs.append(JobSearchResult(
                id=str(row['id']),
                company=row['company'],
                title=row['title'],
                category=row['category'],
                location=row['location'],
                experience=row['experience'],  # career as experienceë¡œ ë§¤í•‘ë¨
                deadline=row['deadline'] if row['deadline'] else None,
                views=row['views'] or 0,
                createdAt=row['created_at'].isoformat() if row['created_at'] else None,
                updatedAt=row['updated_at'].isoformat() if row['updated_at'] else None,
                originalUrl=row['original_url']
            ))
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ê³„ì‚°
        total_pages = (total + limit - 1) // limit
        has_next = page < total_pages
        has_prev = page > 1
        
        pagination = PaginationInfo(
            page=page,
            limit=limit,
            total=total,
            totalPages=total_pages,
            hasNext=has_next,
            hasPrev=has_prev
        )
        
        # í•„í„° ì •ë³´
        filters = {
            "keyword": keyword,
            "category": category,
            "location": location,
            "experience": experience,
            "deadline": deadline,
            "sort": sort
        }
        
        cursor.close()
        conn.close()
        
        return SearchResponse(
            success=True,
            data=jobs,
            pagination=pagination,
            filters=filters
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/search/stats", response_model=StatsResponse)
async def get_stats():
    """í†µê³„ ì •ë³´ ì¡°íšŒ"""
    try:
        conn = get_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # ì „ì²´ ì±„ìš©ê³µê³  ìˆ˜
        cursor.execute("SELECT COUNT(*) as total FROM okky_jobs")
        total_jobs = cursor.fetchone()['total']
        
        # ì˜¤ëŠ˜ ë“±ë¡ëœ ì±„ìš©ê³µê³  ìˆ˜
        today = datetime.now().date()
        cursor.execute("SELECT COUNT(*) as today FROM okky_jobs WHERE DATE(created_at) = %s", (today,))
        today_jobs = cursor.fetchone()['today']
        
        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        cursor.execute("SELECT MAX(updated_at) as last_update FROM okky_jobs")
        last_update = cursor.fetchone()['last_update']
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        cursor.execute("""
            SELECT category, COUNT(*) as count
            FROM okky_jobs
            GROUP BY category
            ORDER BY count DESC
        """)
        category_stats = {row['category']: row['count'] for row in cursor.fetchall()}
        
        stats = {
            "totalJobs": total_jobs,
            "todayJobs": today_jobs,
            "lastUpdate": last_update.isoformat() if last_update else None,
            "categoryStats": category_stats
        }
        
        cursor.close()
        conn.close()
        
        return StatsResponse(success=True, data=stats)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/search/{job_id}", response_model=JobDetailResponse)
async def get_job_detail(job_id: str):
    """ì±„ìš©ê³µê³  ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        conn = get_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT 
                j.id,
                j.company,
                j.title,
                j.category,
                j.location,
                j.career as experience,
                j.deadline,
                COALESCE(d.view_count, 0) as views,
                j.created_at,
                j.updated_at,
                j.link as original_url
            FROM okky_jobs j
            LEFT JOIN okky_job_details d ON j.link = d.link
            WHERE j.id = %s
        """, (job_id,))
        
        job = cursor.fetchone()
        if not job:
            raise HTTPException(status_code=404, detail="ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê¸°ìˆ  ìŠ¤íƒ ì¡°íšŒ (ìƒì„¸ í…Œì´ë¸”ì—ì„œ)
        cursor.execute("""
            SELECT skill
            FROM okky_job_details
            WHERE link = (SELECT link FROM okky_jobs WHERE id = %s)
        """, (job_id,))
        
        tech_stack = [row['skill'] for row in cursor.fetchall() if row['skill']]
        
        # ì—°ë½ì²˜ ì •ë³´ ì¡°íšŒ
        cursor.execute("""
            SELECT c.name, c.phone, c.email
            FROM okky_job_contacts c
            JOIN okky_job_details d ON c.id = d.contact_id
            WHERE d.link = (SELECT link FROM okky_jobs WHERE id = %s)
        """, (job_id,))
        
        contact_row = cursor.fetchone()
        contact = {
            "name": contact_row['name'] if contact_row else None,
            "phone": contact_row['phone'] if contact_row else None,
            "email": contact_row['email'] if contact_row else None
        } if contact_row else {}
        
        # ì¡°íšŒìˆ˜ ì¦ê°€ (okky_job_details í…Œì´ë¸”ì—ì„œ)
        cursor.execute("""
            UPDATE okky_job_details 
            SET view_count = view_count + 1 
            WHERE link = (SELECT link FROM okky_jobs WHERE id = %s)
        """, (job_id,))
        conn.commit()
        
        job_detail = JobDetail(
            id=str(job['id']),
            company=job['company'],
            title=job['title'],
            category=job['category'],
            location=job['location'],
            experience=job['experience'],
            deadline=job['deadline'] if job['deadline'] else None,
            views=(job['views'] or 0) + 1,
            createdAt=job['created_at'].isoformat() if job['created_at'] else None,
            updatedAt=job['updated_at'].isoformat() if job['updated_at'] else None,
            originalUrl=job['original_url'],
            description="",  # ê¸°ë³¸ê°’
            requirements="",  # ê¸°ë³¸ê°’
            techStack=tech_stack,
            contact=contact
        )
        
        cursor.close()
        conn.close()
        
        return JobDetailResponse(success=True, data=job_detail)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@app.get("/jobs/export")
async def export_jobs(keyword: Optional[str] = Query(None)):
    rows = search_jobs(keyword)
    if not rows:
        return JSONResponse(content={"message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}, status_code=404)

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    filename = f"okky_jobs_export_{timestamp}.xlsx".replace(":", "-")
    export_to_excel(rows, filename)

    return FileResponse(
        path=filename,
        filename=filename,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


@app.post("/crawl")
async def manual_crawl():
    """
    ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸
    ë§ˆìŠ¤í„° í¬ë¡¤ë§ â†’ ìƒì„¸ í¬ë¡¤ë§ â†’ DB ì €ì¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
    """
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        def run_crawling():
            try:
                from ..crawler.crawler_master import crawl_all_master_jobs
                from ..crawler.crawler_detail import crawl_detail_jobs
                from ..db.db import save_master_jobs, save_detail_jobs
                
                print("ğŸ•·ï¸ [ìˆ˜ë™ í¬ë¡¤ë§] ë§ˆìŠ¤í„° í¬ë¡¤ë§ ì‹œì‘...")
                master_jobs = crawl_all_master_jobs()
                
                if master_jobs:
                    print(f"âœ… [ìˆ˜ë™ í¬ë¡¤ë§] {len(master_jobs)}ê±´ì˜ ë§ˆìŠ¤í„° ê³µê³  ìˆ˜ì§‘")
                    save_master_jobs(master_jobs)
                    print("âœ… [ìˆ˜ë™ í¬ë¡¤ë§] ë§ˆìŠ¤í„° ê³µê³  DB ì €ì¥ ì™„ë£Œ")
                    
                    print("ğŸ•·ï¸ [ìˆ˜ë™ í¬ë¡¤ë§] ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘...")
                    detail_jobs = crawl_detail_jobs(master_jobs)
                    
                    if detail_jobs:
                        print(f"âœ… [ìˆ˜ë™ í¬ë¡¤ë§] {len(detail_jobs)}ê±´ì˜ ìƒì„¸ ê³µê³  ìˆ˜ì§‘")
                        save_detail_jobs(detail_jobs)
                        print("âœ… [ìˆ˜ë™ í¬ë¡¤ë§] ìƒì„¸ ê³µê³  DB ì €ì¥ ì™„ë£Œ")
                    else:
                        print("âš ï¸ [ìˆ˜ë™ í¬ë¡¤ë§] ìƒì„¸ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    print("âš ï¸ [ìˆ˜ë™ í¬ë¡¤ë§] ë§ˆìŠ¤í„° ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                print("ğŸ‰ [ìˆ˜ë™ í¬ë¡¤ë§] ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ [ìˆ˜ë™ í¬ë¡¤ë§] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        thread = Thread(target=run_crawling)
        thread.daemon = True
        thread.start()
        
        return JSONResponse({
            "message": "í¬ë¡¤ë§ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "status": "started",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "message": f"í¬ë¡¤ë§ ì‹œì‘ ì‹¤íŒ¨: {str(e)}",
                "status": "error",
                "timestamp": datetime.now().isoformat()
            }
        )


@app.get("/crawl/status")
async def crawl_status():
    """
    í¬ë¡¤ë§ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        # ìµœê·¼ í¬ë¡¤ë§ëœ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        conn = get_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)
        
        # ë§ˆìŠ¤í„° ê³µê³  ê°œìˆ˜
        cursor.execute("SELECT COUNT(*) as count FROM okky_jobs")
        master_count = cursor.fetchone()['count']
        
        # ìƒì„¸ ê³µê³  ê°œìˆ˜
        cursor.execute("SELECT COUNT(*) as count FROM okky_job_details")
        detail_count = cursor.fetchone()['count']
        
        # ìµœê·¼ ì—…ë°ì´íŠ¸ ì‹œê°„
        cursor.execute("SELECT MAX(created_at) as last_update FROM okky_jobs")
        last_update = cursor.fetchone()['last_update']
        
        conn.close()
        
        return JSONResponse({
            "master_jobs_count": master_count,
            "detail_jobs_count": detail_count,
            "last_update": last_update.isoformat() if last_update else None,
            "status": "healthy",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "message": f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}",
                "status": "error",
                "timestamp": datetime.now().isoformat()
            }
        )

# ==================== í¬ë¡¤ë§ ë¡œê·¸ ë° íˆìŠ¤í† ë¦¬ API ====================

@app.get("/crawl/logs")
async def get_crawling_logs():
    """ê¸°ë³¸ í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ"""
    try:
        logger = CrawlingLogger()
        logs = logger.get_recent_logs(100)
        
        return {
            "success": True,
            "data": {
                "logs": logs
            }
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": {
                    "code": "LOGS_FETCH_ERROR",
                    "message": "ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "details": str(e)
                }
            }
        )

@app.get("/crawl/logs/realtime")
async def get_realtime_logs():
    """ì‹¤ì‹œê°„ í¬ë¡¤ë§ ë¡œê·¸ ì¡°íšŒ"""
    try:
        logger = CrawlingLogger()
        is_running = logger.is_crawling_running()
        
        if is_running:
            logs = logger.get_recent_logs(50)
            progress = logger.get_current_progress()
            
            return {
                "success": True,
                "data": {
                    "logs": logs,
                    "isRunning": True,
                    "currentProgress": progress
                }
            }
        else:
            return {
                "success": True,
                "data": {
                    "logs": [],
                    "isRunning": False,
                    "currentProgress": 0
                }
            }
            
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": {
                    "code": "REALTIME_LOGS_ERROR",
                    "message": "ì‹¤ì‹œê°„ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "details": str(e)
                }
            }
        )

@app.get("/crawl/history")
async def get_crawling_history():
    """í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        logger = CrawlingLogger()
        history = logger.get_crawling_history(50)
        
        return {
            "success": True,
            "data": {
                "history": history
            }
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": {
                    "code": "HISTORY_FETCH_ERROR",
                    "message": "íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "details": str(e)
                }
            }
        )

@app.post("/crawl/stop")
async def stop_crawling():
    """í¬ë¡¤ë§ ì¤‘ì§€"""
    try:
        logger = CrawlingLogger()
        
        # ì§„í–‰ ì¤‘ì¸ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬ë¥¼ ì‹¤íŒ¨ë¡œ ì—…ë°ì´íŠ¸
        if logger.is_crawling_running():
            logger.update_crawling_history("ì‹¤íŒ¨", 0)
            logger.log_warning("í¬ë¡¤ë§ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": {
                    "code": "CRAWLING_STOP_ERROR",
                    "message": "í¬ë¡¤ë§ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "details": str(e)
                }
            }
        )
